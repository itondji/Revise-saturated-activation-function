{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootcamp_Week1_Idriss Tondji.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhGDogrrCxU_"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoHQN3y3hsQe",
        "outputId": "08fc83ed-d3d1-4691-94c6-c1c503ac1cd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8KGUDEUhy88",
        "outputId": "73e228fa-1e5b-4ec0-efc0-489c49e7a07f"
      },
      "source": [
        "cd drive/MyDrive/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwlioIMQh3f3"
      },
      "source": [
        "# ls"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU7vARlnh7oL"
      },
      "source": [
        "from dlc_practical_prologue import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRBiIUCNGXv"
      },
      "source": [
        "#Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTOiubxWX1nP",
        "outputId": "5162ae15-aeb0-44c1-d0da-985b45834d1d"
      },
      "source": [
        "train_x, train_y, test_x, test_y = load_data(cifar = False, one_hot_labels = True, normalize = True, flatten = True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Using MNIST\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "** Reduce the data-set (use --full for the full thing)\n",
            "** Use 1000 train and 1000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdOZP8CZQitk",
        "outputId": "550ed831-2152-405e-96ba-fe868e1d0d3e"
      },
      "source": [
        "train_x.shape[1], train_y.shape[1]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcY7Qit1jZwK"
      },
      "source": [
        "##1- Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHm9s-QOhMOA"
      },
      "source": [
        "## Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiHRj7LrC7tJ"
      },
      "source": [
        "def sigma(x):\n",
        "  return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "\n",
        "def dsigma(x):\n",
        "  \"\"\"\n",
        "  compute the derivative sigmoid\n",
        "  \"\"\"\n",
        "  return sigma(x) * (1 - sigma(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYV36YPdhQJ6"
      },
      "source": [
        "## Scaled sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6XIcqg350to"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return (4 / (1 + torch.exp(-x))) - 2\n",
        "\n",
        "\n",
        "def dsigmoid(x):\n",
        "  \"\"\"\n",
        "  compute the derivative sigmoid\n",
        "  \"\"\"\n",
        "  return 4 * sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYWKsGIHhF0f"
      },
      "source": [
        "## Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpNck2Y0hA_d"
      },
      "source": [
        "def tanh(x):\n",
        "\n",
        "  return torch.tanh(x)\n",
        "\n",
        "\n",
        "def dtanh(x):\n",
        "  \"\"\"\n",
        "  compute the derivative of tanh function\n",
        "  \"\"\"\n",
        "  return 1 - torch.square(torch.tanh(x))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlXnevrlgkEB"
      },
      "source": [
        "## Penalized tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cScecs97wYXc"
      },
      "source": [
        "def tanh(x , a):\n",
        "  return torch.where(x>0, torch.tanh(x), a*torch.tanh(x))\n",
        "\n",
        "def dtanh(x, a):\n",
        "\n",
        "  \"\"\"\n",
        "  compute the derivative penalized tanh function\n",
        "  \"\"\"\n",
        "  t = torch.tanh(x)\n",
        "  return torch.where(x>0, 1 - t**2, a*(1 - t**2))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWUDRirgpYD"
      },
      "source": [
        "## ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV5aGwVmx060"
      },
      "source": [
        "def Relu(x):\n",
        "  return x*(x > 0)\n",
        "\n",
        "def dRelu(x):\n",
        "  \"\"\"\n",
        "  compute the derivative of ReLU function\n",
        "  \"\"\"\n",
        "  return 1*(x >0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AccwPfKxguT5"
      },
      "source": [
        "## Leaky ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq6BdfK-1mKb"
      },
      "source": [
        "def leakyReLU(x, a):\n",
        "  return torch.where(x>0, x, a * x)\n",
        "\n",
        "def dleakyReLU(x, a):\n",
        "  \"\"\"\n",
        "  compute the derivative of leaky ReLU function\n",
        "  \"\"\"\n",
        "  return torch.where(x > 0, 1., a)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ZTsfE0DUk5",
        "outputId": "55cb9d80-82a2-46cf-f09b-abc1a712fc0f"
      },
      "source": [
        "t1 = torch.tensor([1,2,-30000., -5, -1, 1])\n",
        "dleakyReLU(t1, 0.25)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 0.2500, 0.2500, 0.2500, 1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pNOeze8i4ek"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1eM1cNBHAih"
      },
      "source": [
        "def loss(t , v):\n",
        "  \"\"\"\n",
        "  computes the loss \n",
        "  \"\"\"\n",
        "  return torch.sum((t - v)**2)\n",
        "\n",
        "def dloss(t, v):\n",
        "  \"\"\"\n",
        "  computes the derivative \n",
        "  of loss \n",
        "  \"\"\"\n",
        "  return 2 * (v - t)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X_WdDziH-kS"
      },
      "source": [
        "def forward_pass(w1, b1, w2, b2, x):\n",
        "  \"\"\"\n",
        "  implements the feedforward pass for two layers\n",
        "  \"\"\"\n",
        "\n",
        "  z1 = torch.mm(w1, x) + b1\n",
        "  a1 = tanh(z1, 0.25)\n",
        "  z2 = torch.mm(w2,a1) + b2\n",
        "  a2 = tanh(z2, 0.25)\n",
        "  return z1,a1,z2,a2\n",
        "\n",
        "\n",
        "\n",
        "def backward_passs(w1, b1, w2, b2,t, x, s1, x1, s2, x2, d1_dw1, d1_db1, d1_dw2, d1_db2):\n",
        "  \"\"\"\n",
        "  implements the backforward pass for two layers\n",
        "  \"\"\"\n",
        "\n",
        "  learning_rate = 0.1\n",
        "  N = x.shape[0]\n",
        "  b1 -= 1/N * learning_rate * d1_db1\n",
        "  b2 -= 1/N * learning_rate * d1_db2\n",
        "\n",
        "  w1 -= 1/N * learning_rate * d1_dw1\n",
        "  w2 -= 1/N * learning_rate * d1_dw2\n",
        "\n",
        "  return w1,b1,w2,b2\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvqgKEeHjpcV",
        "outputId": "61263c28-6d96-4ced-be39-b29e4786496c"
      },
      "source": [
        "X = train_x.t()\n",
        "y = train_y.t() * 0.9\n",
        "tst_x = torch.tensor(test_x).t()\n",
        "tst_y = torch.tensor(test_y).t() * 0.9"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di1y9KhPY2U0"
      },
      "source": [
        "import progressbar "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzvrznZLwjqQ"
      },
      "source": [
        "hidden_layer = 50"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVEDActnOnpK"
      },
      "source": [
        "def train_func():\n",
        "\n",
        "  '''\n",
        "    Initialize the parameters:\n",
        "  '''\n",
        "\n",
        "  w1 = torch.empty(hidden_layer, train_x.shape[1]).normal_(mean = 0, std=1e-6)\n",
        "  b1 = torch.empty(hidden_layer).normal_(mean = 0, std=1e-6).reshape(-1,1)\n",
        "  w2 = torch.empty(train_y.shape[1], hidden_layer,).normal_(mean = 0, std=1e-6)\n",
        "  b2 = torch.empty(train_y.shape[1]).normal_(mean = 0, std=1e-6).reshape(-1,1)\n",
        "\n",
        "  dw1 = torch.empty(hidden_layer, train_x.shape[1])\n",
        "  db1 = torch.empty(hidden_layer).reshape(-1,1)\n",
        "  dw1 = torch.empty(hidden_layer, train_x.shape[1])\n",
        "  db2 = torch.empty(train_y.shape[1]).reshape(-1,1)\n",
        "\n",
        "\n",
        "  max_iter = 800\n",
        "  train_errors = []\n",
        "  test_errors  = [] \n",
        "  for i in range(max_iter):\n",
        "    z1, a1, z2, a2 = forward_pass(w1,b1,w2,b2,X)\n",
        "\n",
        "    \n",
        "    da2 = dloss(y, a2)\n",
        "    dz2 = da2 * dtanh(z2, 0.25)\n",
        "\n",
        "    da1 = w2.t().mm(dz2)\n",
        "    dz1 = da1 * dtanh(z1, 0.25)\n",
        "\n",
        "    dw2 = dz2.mm(a1.t())\n",
        "    dw1 = dz1.mm(X.t())\n",
        "    db2 = dz2.sum(dim = 1).reshape(-1,1)\n",
        "    db1 = dz1.sum(dim = 1).reshape(-1,1)\n",
        "    \n",
        "    w1,b1,w2,b2 = backward_passs(w1, b1, w2, b2,y, X, z1, a1, z2, a2, dw1, db1, dw2, db2)\n",
        "\n",
        "    _,_,_,train_predictions = forward_pass(w1,b1,w2,b2,X)\n",
        "    _,_,_,test_predictions  = forward_pass(w1,b1,w2,b2,tst_x)\n",
        "\n",
        "    Train_Accu = torch.sum(train_predictions.argmax(dim=0) == y.argmax(dim=0)) / y.shape[1] * 100\n",
        "    Test_Accu = torch.sum(test_predictions.argmax(dim=0) == t_y.argmax(dim=0)) / t_y.shape[1]  * 100  \n",
        "    \n",
        "    print(\"############## STOP #############\")\n",
        "    print(\"Train_Accuracy at step {} : {}\".format(i, Train_Accu))\n",
        "    print(\"Test_Accuracy error at step {} : {}\".format(i, Test_Accu))\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAZfq0X2rima"
      },
      "source": [
        "train_func()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8ZeKn8ZjVgx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,10), dpi = 40)\n",
        "plt.plot(train_losses)\n",
        "plt.plot(test_losses)\n",
        "plt.legend([\"Train Loss\",\"Test Loss\"])\n",
        "plt.xlabel(\"Number of Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train Loss vs Test Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}